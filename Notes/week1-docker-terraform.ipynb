{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "During the course we will replicate the following architecture:\n",
    "\n",
    "![architecture diagram](https://github.com/DataTalksClub/data-engineering-zoomcamp/raw/main/images/architecture/arch_1.jpg)\n",
    "\n",
    "* [New York's Taxi and Limousine Corporation's Trip Records Dataset](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/dataset.md): the dataset we will use during the course.\n",
    "* [Spark](https://spark.apache.org/): analytics engine for large-scale data processing (distributed processing).\n",
    "* [Google BigQuery](https://cloud.google.com/products/bigquery/): serverless _data warehouse_ (central repository of integrated data from one or more disparate sources).\n",
    "* [Airflow](https://airflow.apache.org/): workflow management platform for data engineering pipelines. In other words, a pipeline orchestration tool.\n",
    "* [Kafka](https://kafka.apache.org/): unified, high-throughput,low-latency platform for handling real-time data feeds (streaming)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pipelines\n",
    "\n",
    "A **data pipeline** is a service that receives data as input and outputs more data. For example, reading a CSV file, transforming the data somehow and storing it as a table in a PostgreSQL database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docker basic concepts\n",
    "\n",
    "_([Video source](https://www.youtube.com/watch?v=EYNwNlOrpr0&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=3))_\n",
    "\n",
    "**Docker** is a _containerization software_ that allows us to isolate software in a similar way to virtual machines but in a much leaner way.\n",
    "\n",
    "A **Docker image** is a _snapshot_ of a container that we can define to run our software, or in this case our data pipelines. By exporting our Docker images to Cloud providers such as Amazon Web Services or Google Cloud Platform we can run our containers there.\n",
    "\n",
    "Docker provides the following advantages:\n",
    "* Reproducibility\n",
    "* Local experimentation\n",
    "* Integration tests (CI/CD)\n",
    "* Running pipelines on the cloud (AWS Batch, Kubernetes jobs)\n",
    "* Spark (analytics engine for large-scale data processing)\n",
    "* Serverless (AWS Lambda, Google functions)\n",
    "\n",
    "Docker containers are ***stateless***: any changes done inside a container will **NOT** be saved when the container is killed and started again. This is an advantage because it allows us to restore any container to its initial state in a reproducible manner, but you will have to store data elsewhere if you need to do so; a common way to do so is with _volumes_.\n",
    "\n",
    ">Note: You may also be interested in a [Docker reference cheatsheet](https://gist.github.com/ziritrion/1842c8a4c4851602a8733bba19ab6050#docker).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a custom pipeline with Docker\n",
    "\n",
    "_([Video source](https://www.youtube.com/watch?v=EYNwNlOrpr0&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=3))_\n",
    "\n",
    "Let's create an example pipeline. We will create a dummy `pipeline.py` Python script that receives an argument and prints it.\n",
    "\n",
    "```python\n",
    "import sys\n",
    "import pandas # we don't need this but it's useful for the example\n",
    "\n",
    "# print arguments\n",
    "print(sys.argv)\n",
    "\n",
    "# argument 0 is the name os the file\n",
    "# argumment 1 contains the actual first argument we care about\n",
    "day = sys.argv[1]\n",
    "\n",
    "# cool pandas stuff goes here\n",
    "\n",
    "# print a sentence with the argument\n",
    "print(f'job finished successfully for day = {day}')\n",
    "```\n",
    "\n",
    "We can run this script with `python pipeline.py <some_number>` and it should print 2 lines:\n",
    "* `['pipeline.py', '<some_number>']`\n",
    "* `job finished successfully for day = <some_number>`\n",
    "\n",
    "Let's containerize it by creating a Docker image. Create the folllowing `Dockerfile` file:\n",
    "\n",
    "```dockerfile\n",
    "# base Docker image that we will build on\n",
    "FROM python:3.9.1\n",
    "\n",
    "# set up our image by installing prerequisites; pandas in this case\n",
    "RUN pip install pandas\n",
    "\n",
    "# set up the working directory inside the container\n",
    "WORKDIR /app\n",
    "# copy the script to the container. 1st name is source file, 2nd is destination\n",
    "COPY pipeline.py pipeline.py\n",
    "\n",
    "# define what to do first when the container runs\n",
    "# in this example, we will just run the script\n",
    "ENTRYPOINT [\"python\", \"pipeline.py\"]\n",
    "```\n",
    "\n",
    "Let's build the image:\n",
    "\n",
    "\n",
    "```ssh\n",
    "docker build -t test:pandas .\n",
    "```\n",
    "* The image name will be `test` and its tag will be `pandas`. If the tag isn't specified it will default to `latest`.\n",
    "\n",
    "We can now run the container and pass an argument to it, so that our pipeline will receive it:\n",
    "\n",
    "```ssh\n",
    "docker run -it test:pandas some_number\n",
    "```\n",
    "\n",
    "You should get the same output you did when you ran the pipeline script by itself.\n",
    "\n",
    ">Note: these instructions asume that `pipeline.py` and `Dockerfile` are in the same directory. The Docker commands should also be run from the same directory as these files.\n",
    "\n",
    "## Running Postgres in a container\n",
    "\n",
    "_([Video source](https://www.youtube.com/watch?v=2JM-ziJt0WI&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=4))_\n",
    "\n",
    "In later parts of the course we will use Airflow, which uses PostgreSQL internally. For simpler tests we can use PostgreSQL (or just Postgres) directly.\n",
    "\n",
    "You can run a containerized version of Postgres that doesn't require any installation steps. You only need to provide a few _environment variables_ to it as well as a _volume_ for storing data.\n",
    "\n",
    "Create a folder anywhere you'd like for Postgres to store data in. We will use the example folder `ny_taxi_postgres_data`. Here's how to run the container:\n",
    "\n",
    "```bash\n",
    "docker run -it \\\n",
    "    -e POSTGRES_USER=\"root\" \\\n",
    "    -e POSTGRES_PASSWORD=\"root\" \\\n",
    "    -e POSTGRES_DB=\"ny_taxi\" \\\n",
    "    -v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\n",
    "    -p 5432:5432 \\\n",
    "    postgres:13\n",
    "```\n",
    "* The container needs 3 environment variables:\n",
    "    * `POSTGRES_USER` is the username for logging into the database. We chose `root`.\n",
    "    * `POSTGRES_PASSWORD` is the password for the database. We chose `root`\n",
    "        * ***IMPORTANT: These values are only meant for testing. Please change them for any serious project.***\n",
    "    * `POSTGRES_DB` is the name that we will give the database. We chose `ny_taxi`.\n",
    "* `-v` points to the volume directory. The colon `:` separates the first part (path to the folder in the host computer) from the second part (path to the folder inside the container).\n",
    "    * Path names must be absolute. If you're in a UNIX-like system, you can use `pwd` to print you local folder as a shortcut; this example should work with both `bash` and `zsh` shells, but `fish` will require you to remove the `$`.\n",
    "    * This command will only work if you run it from a directory which contains the `ny_taxi_postgres_data` subdirectory you created above.\n",
    "* The `-p` is for port mapping. We map the default Postgres port to the same port in the host.\n",
    "* The last argument is the image name and tag. We run the official `postgres` image on its version `13`.\n",
    "\n",
    "Once the container is running, we can log into our database with [pgcli](https://www.pgcli.com/) with the following command:\n",
    "\n",
    "```bash\n",
    "pgcli -h localhost -p 5432 -u root -d ny_taxi\n",
    "```\n",
    "* `-h` is the host. Since we're running locally we can use `localhost`.\n",
    "* `-p` is the port.\n",
    "* `-u` is the username.\n",
    "* `-d` is the database name.\n",
    "* The password is not provided; it will be requested after running the command.\n",
    "\n",
    "## Ingesting data to Postgres with Python\n",
    "\n",
    "_([Video source](https://www.youtube.com/watch?v=2JM-ziJt0WI&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=4))_\n",
    "\n",
    "We will now create a Jupyter Notebook `upload-data.ipynb` file which we will use to read a CSV file and export it to Postgres.\n",
    "\n",
    "We will use data from the [NYC TLC Trip Record Data website](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page). Specifically, we will use the [Yellow taxi trip records CSV file for January 2021](https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2021-01.csv). A dictionary to understand each field is available [here](https://www1.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf).\n",
    "\n",
    ">Note: knowledge of Jupyter Notebook, Python environment management and Pandas is asumed in these notes. Please check [this link](https://gist.github.com/ziritrion/9b80e47956adc0f20ecce209d494cd0a#pandas) for a Pandas cheatsheet and [this link](https://gist.github.com/ziritrion/8024025672ea92b8bdeb320d6015aa0d) for a Conda cheatsheet for Python environment management.\n",
    "\n",
    "Check the completed `upload-data.ipynb` [in this link](../1_intro/upload-data.ipynb) for a detailed guide. Feel free to copy the file to your work directory; in the same directory you will need to have the CSV file linked above and the `ny_taxi_postgres_data` subdirectory.\n",
    "\n",
    "## Connecting pgAdmin and Postgres with Docker networking\n",
    "\n",
    "_([Video source](https://www.youtube.com/watch?v=hCAIVe9N0ow&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=5))_\n",
    "\n",
    "`pgcli` is a handy tool but it's cumbersome to use. [`pgAdmin` is a web-based tool](https://www.pgadmin.org/) that makes it more convenient to access and manage our databases. It's possible to run pgAdmin as as container along with the Postgres container, but both containers will have to be in the same _virtual network_ so that they can find each other.\n",
    "\n",
    "Let's create a virtual Docker network called `pg-network`:\n",
    "\n",
    "```bash\n",
    "docker network create pg-network\n",
    "```\n",
    "\n",
    ">You can remove the network later with the command `docker network rm pg-network` . You can look at the existing networks with `docker network ls` .\n",
    "\n",
    "We will now re-run our Postgres container with the added network name and the container network name, so that the pgAdmin container can find it (we'll use `pg-database` for the container name):\n",
    "\n",
    "```bash\n",
    "docker run -it \\\n",
    "    -e POSTGRES_USER=\"root\" \\\n",
    "    -e POSTGRES_PASSWORD=\"root\" \\\n",
    "    -e POSTGRES_DB=\"ny_taxi\" \\\n",
    "    -v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\n",
    "    -p 5432:5432 \\\n",
    "    --network=pg-network \\\n",
    "    --name pg-database \\\n",
    "    postgres:13\n",
    "```\n",
    "\n",
    "We will now run the pgAdmin container on another terminal:\n",
    "\n",
    "```bash\n",
    "docker run -it \\\n",
    "    -e PGADMIN_DEFAULT_EMAIL=\"admin@admin.com\" \\\n",
    "    -e PGADMIN_DEFAULT_PASSWORD=\"root\" \\\n",
    "    -p 8080:80 \\\n",
    "    --network=pg-network \\\n",
    "    --name pgadmin \\\n",
    "    dpage/pgadmin4\n",
    "```\n",
    "* The container needs 2 environment variables: a login email and a password. We use `admin@admin.com` and `root` in this example.\n",
    " * ***IMPORTANT: these are example values for testing and should never be used on production. Change them accordingly when needed.***\n",
    "* pgAdmin is a web app and its default port is 80; we map it to 8080 in our localhost to avoid any possible conflicts.\n",
    "* Just like with the Postgres container, we specify a network and a name. However, the name in this example isn't really necessary because there won't be any containers trying to access this particular container.\n",
    "* The actual image name is `dpage/pgadmin4` .\n",
    "\n",
    "You should now be able to load pgAdmin on a web browser by browsing to `localhost:8080`. Use the same email and password you used for running the container to log in.\n",
    "\n",
    "Right-click on _Servers_ on the left sidebar and select _Create_ > _Server..._\n",
    "\n",
    "![steps](images/01_02.png)\n",
    "\n",
    "Under _General_ give the Server a name and under _Connection_ add the same host name, user and password you used when running the container.\n",
    "\n",
    "![steps](images/01_03.png)\n",
    "![steps](images/01_04.png)\n",
    "\n",
    "Click on _Save_. You should now be connected to the database.\n",
    "\n",
    "We will explore using pgAdmin in later lessons.\n",
    "\n",
    "\n",
    "## Using the ingestion script with Docker\n",
    "\n",
    "_([Video source](https://www.youtube.com/watch?v=B1WwATwf-vY&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=8))_\n",
    "\n",
    "We will now export the Jupyter notebook file to a regular Python script and use Docker to run it.\n",
    "\n",
    "### Exporting and testing the script\n",
    "\n",
    "You can export the `ipynb` file to `py` with this command:\n",
    "\n",
    "```bash\n",
    "jupyter nbconvert --to=script upload-data.ipynb\n",
    "```\n",
    "\n",
    "Clean up the script by removing everything we don't need. We will also rename it to `ingest_data.py` and add a few modifications:\n",
    "* We will use [argparse](https://docs.python.org/3/library/argparse.html) to handle the following command line arguments:\n",
    "    * Username\n",
    "    * Password\n",
    "    * Host\n",
    "    * Port\n",
    "    * Database name\n",
    "    * Table name\n",
    "    * URL for the CSV file\n",
    "* The _engine_ we created for connecting to Postgres will be tweaked so that we pass the parameters and build the URL from them, like this:\n",
    "    ```python\n",
    "    engine = create_engine(f'postgresql://{user}:{password}@{host}:{port}/{db}')\n",
    "    ```\n",
    "* We will also download the CSV using the provided URL argument.\n",
    "\n",
    "You can check the completed `ingest_data.py` script [in this link](../1_intro/ingest_data.py).\n",
    "\n",
    "In order to test the script we will have to drop the table we previously created. In pgAdmin, in the sidebar navigate to _Servers > Docker localhost > Databases > ny_taxi > Schemas > public > Tables > yellow_taxi_data_, right click on _yellow_taxi_data_ and select _Query tool_. Introduce the following command:\n",
    "\n",
    "```sql\n",
    "DROP TABLE yellow_taxi_data;\n",
    "```\n",
    "\n",
    "We are now ready to test the script with the following command:\n",
    "\n",
    "```bash\n",
    "python ingest_data.py \\\n",
    "    --user=root \\\n",
    "    --password=root \\\n",
    "    --host=localhost \\\n",
    "    --port=5432 \\\n",
    "    --db=ny_taxi \\\n",
    "    --table_name=yellow_taxi_trips \\\n",
    "    --url=\"https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2021-01.csv\"\n",
    "```\n",
    "* Note that we've changed the table name from `yellow_taxi_data` to `yellow_taxi_trips`.\n",
    "\n",
    "Back in pgAdmin, refresh the Tables and check that `yellow_taxi_trips` was created. You can also run a SQL query to check the contents:\n",
    "\n",
    "```sql\n",
    "SELECT\n",
    "    COUNT(1)\n",
    "FROM\n",
    "    yellow_taxi_trips;\n",
    "```\n",
    "* This query should return 1,369,765 rows.\n",
    "\n",
    "### Dockerizing the script\n",
    "\n",
    "Let's modify the [Dockerfile we created before](#creating-a-custom-pipeline-with-docker) to include our `ingest_data.py` script and create a new image:\n",
    "\n",
    "```dockerfile\n",
    "FROM python:3.9.1\n",
    "\n",
    "# We need to install wget to download the csv file\n",
    "RUN apt-get install wget\n",
    "# psycopg2 is a postgres db adapter for python: sqlalchemy needs it\n",
    "RUN pip install pandas sqlalchemy psycopg2\n",
    "\n",
    "WORKDIR /app\n",
    "COPY ingest_data.py ingest_data.py \n",
    "\n",
    "ENTRYPOINT [ \"python\", \"ingest_data.py\" ]\n",
    "```\n",
    "\n",
    "Build the image:\n",
    "```bash\n",
    "docker build -t taxi_ingest:v001 .\n",
    "```\n",
    "\n",
    "And run it:\n",
    "```bash\n",
    "docker run -it \\\n",
    "    --network=pg-network \\\n",
    "    taxi_ingest:v001 \\\n",
    "    --user=root \\\n",
    "    --password=root \\\n",
    "    --host=pg-database \\\n",
    "    --port=5432 \\\n",
    "    --db=ny_taxi \\\n",
    "    --table_name=yellow_taxi_trips \\\n",
    "    --url=\"https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2021-01.csv\"\n",
    "```\n",
    "* We need to provide the network for Docker to find the Postgres container. It goes before the name of the image.\n",
    "* Since Postgres is running on a separate container, the host argument will have to point to the container name of Postgres.\n",
    "* You can drop the table in pgAdmin beforehand if you want, but the script will automatically replace the pre-existing table.\n",
    "\n",
    "## Running Postgres and pgAdmin with Docker-compose\n",
    "\n",
    "_([Video source](https://www.youtube.com/watch?v=hKI6PkPhpa0&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=7))_\n",
    "\n",
    "`docker-compose` allows us to launch multiple containers using a single configuration file, so that we don't have to run multiple complex `docker run` commands separately.\n",
    "\n",
    "Docker compose makes use of YAML files. Here's the `docker-compose.yaml` file for running the Postgres and pgAdmin containers:\n",
    "\n",
    "```yaml\n",
    "services:\n",
    "  pgdatabase:\n",
    "    image: postgres:13\n",
    "    environment:\n",
    "      - POSTGRES_USER=root\n",
    "      - POSTGRES_PASSWORD=root\n",
    "      - POSTGRES_DB=ny_taxi\n",
    "    volumes:\n",
    "      - \"./ny_taxi_postgres_data:/var/lib/postgresql/data:rw\"\n",
    "    ports:\n",
    "      - \"5432:5432\"\n",
    "  pgadmin:\n",
    "    image: dpage/pgadmin4\n",
    "    environment:\n",
    "      - PGADMIN_DEFAULT_EMAIL=admin@admin.com\n",
    "      - PGADMIN_DEFAULT_PASSWORD=root\n",
    "    volumes:\n",
    "      - \"./data_pgadmin:/var/lib/pgadmin\"\n",
    "    ports:\n",
    "      - \"8080:80\"\n",
    "```\n",
    "* We don't have to specify a network because `docker-compose` takes care of it: every single container (or \"service\", as the file states) will run withing the same network and will be able to find each other according to their names (`pgdatabase` and `pgadmin` in this example).\n",
    "* We've added a volume for pgAdmin to save its settings, so that you don't have to keep re-creating the connection to Postgres every time ypu rerun the container. Make sure you create a `data_pgadmin` directory in your work folder where you run `docker-compose` from.\n",
    "* All other details from the `docker run` commands (environment variables, volumes and ports) are mentioned accordingly in the file following YAML syntax.\n",
    "\n",
    "We can now run Docker compose by running the following command from the same directory where `docker-compose.yaml` is found. Make sure that all previous containers aren't running anymore:\n",
    "\n",
    "```bash\n",
    "docker-compose up\n",
    "```\n",
    "\n",
    ">Note: this command asumes that the `ny_taxi_postgres_data` used for mounting the volume is in the same directory as `docker-compose.yaml`.\n",
    "\n",
    "Since the settings for pgAdmin were stored within the container and we have killed the previous onem you will have to re-create the connection by following the steps [in this section](#connecting-pgadmin-and-postgres-with-docker-networking).\n",
    "\n",
    "You will have to press `Ctrl+C` in order to shut down the containers. The proper way of shutting them down is with this command:\n",
    "\n",
    "```bash\n",
    "docker-compose down\n",
    "```\n",
    "\n",
    "And if you want to run the containers again in the background rather than in the foreground (thus freeing up your terminal), you can run them in detached mode:\n",
    "\n",
    "```bash\n",
    "docker-compose up -d\n",
    "```\n",
    "\n",
    "If you want to re-run the dockerized ingest script when you run Postgres and pgAdmin with `docker-compose`, you will have to find the name of the virtual network that Docker compose created for the containers. You can use the command `docker network ls` to find it and then change the `docker run` command for the dockerized script to include the network name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这套流程的核心目标是**使用 Docker 构建一个数据管道**，实现**数据的提取、存储和管理**，具体包括以下几个部分：\n",
    "\n",
    "1. **创建一个基本的数据处理脚本**（`pipeline.py`），并将其容器化（Dockerfile）。  \n",
    "   - 这个脚本接受参数，并模拟数据处理任务。\n",
    "   - 通过 Docker 运行这个脚本，验证它可以接收参数并正确运行。\n",
    "\n",
    "2. **使用 Docker 运行一个 PostgreSQL 数据库**，用于存储数据。  \n",
    "   - 通过 `docker run` 启动 Postgres 容器，并挂载一个本地目录用于持久化数据存储。\n",
    "   - 允许外部工具（如 `pgcli` 或 `pgAdmin`）连接数据库进行管理。\n",
    "\n",
    "3. **使用 Python 读取 CSV 数据并导入到 Postgres**。  \n",
    "   - 下载纽约出租车数据（2021年1月）。\n",
    "   - 编写 `ingest_data.py`，使用 `argparse` 处理命令行参数，并将数据写入 Postgres。\n",
    "   - 先本地测试 `ingest_data.py`，确保能正确连接数据库并导入数据。\n",
    "\n",
    "4. **容器化数据导入脚本**，以便用 Docker 运行它。  \n",
    "   - 在 Dockerfile 中安装 `pandas`、`sqlalchemy` 和 `psycopg2`。\n",
    "   - 运行容器时传入数据库连接参数，确保它可以从 URL 下载 CSV 并导入到 Postgres。\n",
    "\n",
    "5. **使用 Docker 网络连接 Postgres 和 pgAdmin**，以便可视化数据库。  \n",
    "   - 创建 `pg-network`，让 Postgres 和 pgAdmin 运行在同一个 Docker 网络中。\n",
    "   - 运行 pgAdmin，配置连接，使用图形界面管理数据库。\n",
    "\n",
    "6. **使用 Docker Compose 简化整个流程**。  \n",
    "   - `docker-compose.yaml` 定义了 Postgres 和 pgAdmin 的服务。\n",
    "   - 只需要运行 `docker-compose up`，就能同时启动 Postgres 和 pgAdmin，而不必手动运行多个 `docker run` 命令。\n",
    "   - 通过 `docker-compose down` 关闭所有容器。\n",
    "\n",
    "### 目的\n",
    "- **简化数据处理流程**：使用 Docker 容器化 Python 任务，提高可移植性。\n",
    "- **自动化数据库管理**：PostgreSQL 以容器方式运行，持久化数据，支持远程访问。\n",
    "- **构建可复用的数据管道**：数据可以自动下载、处理并存入数据库，支持后续分析。\n",
    "- **使用 Docker Compose 提高可维护性**：避免手动启动多个容器，确保服务可以一致运行。\n",
    "\n",
    "### 总结\n",
    "这个流程就是**用 Docker 构建一个端到端的数据处理管道**，从**获取数据 → 处理数据 → 存入数据库 → 通过 pgAdmin 进行管理**，并最终通过 `docker-compose` 进行简化和自动化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
